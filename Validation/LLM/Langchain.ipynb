{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T06:31:02.989242Z",
     "start_time": "2025-01-22T06:30:58.219269Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "\n",
    "os.environ[\"DeepSeek_API_KEY\"] = 'sk-61de429c43324f1e90a41e8610637a5e'\n",
    "\n",
    "\n",
    "from langchain.llms import OpenAI,ZhipuAI\n",
    "llm  = OpenAI()\n",
    "llm.predict('ç»™æˆ‘ä¸€ä¸ªå¥½å…»çš„å­©å­çš„åå­—')"
   ],
   "id": "743462aa967ae94d",
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRateLimitError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mlangchain\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mllms\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m OpenAI\n\u001B[0;32m      9\u001B[0m llm  \u001B[38;5;241m=\u001B[39m OpenAI()\n\u001B[1;32m---> 10\u001B[0m \u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mç»™æˆ‘ä¸€ä¸ªå¥½å…»çš„å­©å­çš„åå­—\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:182\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    180\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    181\u001B[0m     emit_warning()\n\u001B[1;32m--> 182\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1335\u001B[0m, in \u001B[0;36mBaseLLM.predict\u001B[1;34m(self, text, stop, **kwargs)\u001B[0m\n\u001B[0;32m   1330\u001B[0m \u001B[38;5;129m@deprecated\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.1.7\u001B[39m\u001B[38;5;124m\"\u001B[39m, alternative\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minvoke\u001B[39m\u001B[38;5;124m\"\u001B[39m, removal\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1.0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1331\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpredict\u001B[39m(\n\u001B[0;32m   1332\u001B[0m     \u001B[38;5;28mself\u001B[39m, text: \u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m*\u001B[39m, stop: Optional[Sequence[\u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any\n\u001B[0;32m   1333\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[0;32m   1334\u001B[0m     _stop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m stop \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(stop)\n\u001B[1;32m-> 1335\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_stop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:182\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    180\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    181\u001B[0m     emit_warning()\n\u001B[1;32m--> 182\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1297\u001B[0m, in \u001B[0;36mBaseLLM.__call__\u001B[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001B[0m\n\u001B[0;32m   1290\u001B[0m     msg \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   1291\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1292\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(prompt)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. If you want to run the LLM on multiple prompts, use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1293\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`generate` instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1294\u001B[0m     )\n\u001B[0;32m   1295\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n\u001B[0;32m   1296\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m-> 1297\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1298\u001B[0m \u001B[43m        \u001B[49m\u001B[43m[\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1299\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1300\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1301\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1302\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1303\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1304\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1305\u001B[0m     \u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1306\u001B[0m     \u001B[38;5;241m.\u001B[39mtext\n\u001B[0;32m   1307\u001B[0m )\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:962\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    947\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m get_llm_cache() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m    948\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    949\u001B[0m         callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[0;32m    950\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_serialized,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    960\u001B[0m         )\n\u001B[0;32m    961\u001B[0m     ]\n\u001B[1;32m--> 962\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    963\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    964\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    965\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[0;32m    966\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:796\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[0;32m    794\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n\u001B[0;32m    795\u001B[0m         run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[1;32m--> 796\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    797\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[0;32m    798\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m manager, flattened_output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(run_managers, flattened_outputs):\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:783\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[0;32m    773\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[0;32m    774\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    775\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    779\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    780\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    781\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    782\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m--> 783\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    784\u001B[0m \u001B[43m                \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    785\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    786\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# TODO: support multiple run managers\u001B[39;49;00m\n\u001B[0;32m    787\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    788\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    789\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    790\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[0;32m    791\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[0;32m    792\u001B[0m         )\n\u001B[0;32m    793\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    794\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\langchain_community\\llms\\openai.py:463\u001B[0m, in \u001B[0;36mBaseOpenAI._generate\u001B[1;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    451\u001B[0m     choices\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m    452\u001B[0m         {\n\u001B[0;32m    453\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m: generation\u001B[38;5;241m.\u001B[39mtext,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    460\u001B[0m         }\n\u001B[0;32m    461\u001B[0m     )\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 463\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mcompletion_with_retry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    464\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_prompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\n\u001B[0;32m    465\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    466\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[0;32m    467\u001B[0m         \u001B[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001B[39;00m\n\u001B[0;32m    468\u001B[0m         \u001B[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001B[39;00m\n\u001B[0;32m    469\u001B[0m         response \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mdict()\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\langchain_community\\llms\\openai.py:121\u001B[0m, in \u001B[0;36mcompletion_with_retry\u001B[1;34m(llm, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001B[39;00m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_openai_v1():\n\u001B[1;32m--> 121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    123\u001B[0m retry_decorator \u001B[38;5;241m=\u001B[39m _create_retry_decorator(llm, run_manager\u001B[38;5;241m=\u001B[39mrun_manager)\n\u001B[0;32m    125\u001B[0m \u001B[38;5;129m@retry_decorator\u001B[39m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_completion_with_retry\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:275\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    273\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    274\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 275\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\openai\\resources\\completions.py:539\u001B[0m, in \u001B[0;36mCompletions.create\u001B[1;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    510\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    512\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    537\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[0;32m    538\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Completion \u001B[38;5;241m|\u001B[39m Stream[Completion]:\n\u001B[1;32m--> 539\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    540\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    541\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    542\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m    543\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    544\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprompt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    545\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbest_of\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    546\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mecho\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mecho\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    547\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    548\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    549\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    550\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    551\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    552\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    553\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    554\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    555\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    556\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    557\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msuffix\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43msuffix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    558\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    559\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    560\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    561\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    562\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    563\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    564\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    565\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[0;32m    566\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    567\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    568\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    569\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mCompletion\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    570\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\openai\\_base_client.py:1280\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1266\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1267\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1268\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1275\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1276\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1277\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1278\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1279\u001B[0m     )\n\u001B[1;32m-> 1280\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\openai\\_base_client.py:957\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    954\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    955\u001B[0m     retries_taken \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m--> 957\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    958\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    959\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    960\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    961\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    962\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries_taken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    963\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\openai\\_base_client.py:1046\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1044\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m remaining_retries \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_retry(err\u001B[38;5;241m.\u001B[39mresponse):\n\u001B[0;32m   1045\u001B[0m     err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m-> 1046\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retry_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1047\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1048\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1049\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries_taken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1050\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresponse_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1051\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1052\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1053\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1055\u001B[0m \u001B[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001B[39;00m\n\u001B[0;32m   1056\u001B[0m \u001B[38;5;66;03m# to completion before attempting to access the response text.\u001B[39;00m\n\u001B[0;32m   1057\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mis_closed:\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\openai\\_base_client.py:1095\u001B[0m, in \u001B[0;36mSyncAPIClient._retry_request\u001B[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1091\u001B[0m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[0;32m   1092\u001B[0m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[0;32m   1093\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(timeout)\n\u001B[1;32m-> 1095\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1096\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1097\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1098\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries_taken\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1099\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1100\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1101\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\openai\\_base_client.py:1046\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1044\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m remaining_retries \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_retry(err\u001B[38;5;241m.\u001B[39mresponse):\n\u001B[0;32m   1045\u001B[0m     err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m-> 1046\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retry_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1047\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1048\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1049\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries_taken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1050\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresponse_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1051\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1052\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1053\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1055\u001B[0m \u001B[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001B[39;00m\n\u001B[0;32m   1056\u001B[0m \u001B[38;5;66;03m# to completion before attempting to access the response text.\u001B[39;00m\n\u001B[0;32m   1057\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mis_closed:\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\openai\\_base_client.py:1095\u001B[0m, in \u001B[0;36mSyncAPIClient._retry_request\u001B[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1091\u001B[0m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[0;32m   1092\u001B[0m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[0;32m   1093\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(timeout)\n\u001B[1;32m-> 1095\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1096\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1097\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1098\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries_taken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries_taken\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1099\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1100\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1101\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\workspace\\LMMeasure\\venv\\Lib\\site-packages\\openai\\_base_client.py:1061\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1058\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m   1060\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1061\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1063\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[0;32m   1064\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m   1065\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1069\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39mretries_taken,\n\u001B[0;32m   1070\u001B[0m )\n",
      "\u001B[1;31mRateLimitError\u001B[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# æ™ºè°± + langchain",
   "id": "e16409189b4ee4f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T06:57:49.980296Z",
     "start_time": "2025-01-22T06:57:40.987268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.95,\n",
    "    model=\"glm-4-plus\",\n",
    "    openai_api_key=\"b39529e090954a2496d240535200e2d3.x93fykyPxeiRroto\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "conversation.invoke({\"question\": \"ç»™æˆ‘ä¸€ä¸ªå¥½å…»çš„å­©å­çš„åå­—\"})"
   ],
   "id": "b940945eae5e80f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: ç»™æˆ‘ä¸€ä¸ªå¥½å…»çš„å­©å­çš„åå­—\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'ç»™æˆ‘ä¸€ä¸ªå¥½å…»çš„å­©å­çš„åå­—',\n",
       " 'chat_history': [HumanMessage(content='ç»™æˆ‘ä¸€ä¸ªå¥½å…»çš„å­©å­çš„åå­—', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='å½“ç„¶å¯ä»¥ï¼é€‰æ‹©ä¸€ä¸ªå¥½å…»çš„åå­—ï¼Œé€šå¸¸æ„å‘³ç€è¿™ä¸ªåå­—æ—¢å¥½å¬åˆå®¹æ˜“è¯»å†™ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å»ºè®®ï¼Œé€‚åˆå„ç§æ–‡åŒ–èƒŒæ™¯ï¼š\\n\\n**ç”·å­©åå­—ï¼š**\\n1. **å°æ˜** - ç®€å•æ˜“è®°ï¼Œåœ¨ä¸­å›½éå¸¸å¸¸è§ã€‚\\n2. **Leo** - å›½é™…åŒ–ï¼Œå‘éŸ³ç®€å•ã€‚\\n3. **Tom** - ç»å…¸ä¸”å®¹æ˜“æ‹¼å†™ã€‚\\n4. **Jack** - ç®€çŸ­æœ‰åŠ›ï¼Œå¹¿æ³›ä½¿ç”¨ã€‚\\n5. **Alex** - å¤šç§æ–‡åŒ–ä¸­éƒ½å¾ˆæµè¡Œã€‚\\n\\n**å¥³å­©åå­—ï¼š**\\n1. **å°ä¸½** - å¬èµ·æ¥æ¸©æŸ”ï¼Œä¹¦å†™ç®€å•ã€‚\\n2. **Lily** - èŠ±åï¼Œä¼˜ç¾ä¸”æ˜“è®°ã€‚\\n3. **Mia** - ç®€çŸ­å¯çˆ±ï¼Œå¤šæ–‡åŒ–é€šç”¨ã€‚\\n4. **Sophie** - å¬èµ·æ¥ä¼˜é›…ï¼Œä¹¦å†™ä¹Ÿä¸å¤æ‚ã€‚\\n5. **Emma** - ç»å…¸ä¸”æµè¡Œã€‚\\n\\nå½“ç„¶ï¼Œé€‰æ‹©åå­—ä¹Ÿè¦è€ƒè™‘å®¶åº­çš„æ–‡åŒ–èƒŒæ™¯å’Œä¸ªäººå–œå¥½ã€‚å¸Œæœ›è¿™äº›å»ºè®®å¯¹ä½ æœ‰å¸®åŠ©ï¼å¦‚æœæœ‰å…¶ä»–è¦æ±‚æˆ–éœ€è¦æ›´å¤šå»ºè®®ï¼Œéšæ—¶å‘Šè¯‰æˆ‘å“¦ï¼ğŸ˜Š', additional_kwargs={}, response_metadata={})],\n",
       " 'text': 'å½“ç„¶å¯ä»¥ï¼é€‰æ‹©ä¸€ä¸ªå¥½å…»çš„åå­—ï¼Œé€šå¸¸æ„å‘³ç€è¿™ä¸ªåå­—æ—¢å¥½å¬åˆå®¹æ˜“è¯»å†™ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å»ºè®®ï¼Œé€‚åˆå„ç§æ–‡åŒ–èƒŒæ™¯ï¼š\\n\\n**ç”·å­©åå­—ï¼š**\\n1. **å°æ˜** - ç®€å•æ˜“è®°ï¼Œåœ¨ä¸­å›½éå¸¸å¸¸è§ã€‚\\n2. **Leo** - å›½é™…åŒ–ï¼Œå‘éŸ³ç®€å•ã€‚\\n3. **Tom** - ç»å…¸ä¸”å®¹æ˜“æ‹¼å†™ã€‚\\n4. **Jack** - ç®€çŸ­æœ‰åŠ›ï¼Œå¹¿æ³›ä½¿ç”¨ã€‚\\n5. **Alex** - å¤šç§æ–‡åŒ–ä¸­éƒ½å¾ˆæµè¡Œã€‚\\n\\n**å¥³å­©åå­—ï¼š**\\n1. **å°ä¸½** - å¬èµ·æ¥æ¸©æŸ”ï¼Œä¹¦å†™ç®€å•ã€‚\\n2. **Lily** - èŠ±åï¼Œä¼˜ç¾ä¸”æ˜“è®°ã€‚\\n3. **Mia** - ç®€çŸ­å¯çˆ±ï¼Œå¤šæ–‡åŒ–é€šç”¨ã€‚\\n4. **Sophie** - å¬èµ·æ¥ä¼˜é›…ï¼Œä¹¦å†™ä¹Ÿä¸å¤æ‚ã€‚\\n5. **Emma** - ç»å…¸ä¸”æµè¡Œã€‚\\n\\nå½“ç„¶ï¼Œé€‰æ‹©åå­—ä¹Ÿè¦è€ƒè™‘å®¶åº­çš„æ–‡åŒ–èƒŒæ™¯å’Œä¸ªäººå–œå¥½ã€‚å¸Œæœ›è¿™äº›å»ºè®®å¯¹ä½ æœ‰å¸®åŠ©ï¼å¦‚æœæœ‰å…¶ä»–è¦æ±‚æˆ–éœ€è¦æ›´å¤šå»ºè®®ï¼Œéšæ—¶å‘Šè¯‰æˆ‘å“¦ï¼ğŸ˜Š'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
